{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "In C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from pylab import rcParams\n",
    "from keras.layers import Activation, Dense,Dot,merge, Dropout,Lambda,Permute,Multiply,LSTM, Conv2D, Flatten, MaxPooling2D, LSTM,RepeatVector,Reshape,TimeDistributed,UpSampling1D\n",
    "# import common as com\n",
    "import keras.models\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, BatchNormalization, Activation\n",
    "from keras.models import Model\n",
    "# from keras.layers.wrappers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 128\n",
    "height = 64\n",
    "channels = 1\n",
    "input_dim = 640\n",
    "timesteps = 10\n",
    "n_features = 64\n",
    "lr = 0.001\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "numpyA = np.ones([1,64])\n",
    "\n",
    "def multA(x):\n",
    "    A = K.variable(np.ones([1,64]))\n",
    "\n",
    "    return K.dot(x,A)\n",
    "# def multa(x)\n",
    "def multA_T(x):\n",
    "    A = K.variable(np.ones([1,10]))\n",
    "\n",
    "    return K.dot(x,A)\n",
    "# weight_2 = Lambda(lambda x:x*0.2)\n",
    "# weight_gru1 = weight_2(gru1)\n",
    "# output = Lambda(lambda x: K.sum(x, axis=-1))(vec)\n",
    "# model.add(Lambda(multA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------#\n",
    "#   注意力模块\n",
    "#-------------------------------------------#\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, lstm_units)\n",
    "\n",
    "    # (batch_size, time_steps, lstm_units) -> (batch_size, lstm_units, time_steps)\n",
    "    a = Permute((2, 1))(inputs)\n",
    "\n",
    "    # 对最后一维进行全连接\n",
    "    # (batch_size, lstm_units, time_steps) -> (batch_size, lstm_units, time_steps)\n",
    "    a = Dense(10, activation='softmax')(a)\n",
    "\n",
    "    # (batch_size, lstm_units, time_steps) -> (batch_size, time_steps, lstm_units)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "\n",
    "    # 相乘\n",
    "    # 相当于获得每一个step中，每个维度在所有step中的权重\n",
    "    output_attention_mul = Multiply()([inputs,a_probs])\n",
    "#     output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 640)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 10, 64)       0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 10, 64)       4160        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 10, 64)       0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 10)           0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 10, 1)        0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 10, 64)       0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 10, 64)       0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 10, 64)       0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 10, 64)       0           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 10, 64)       0           lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 640)          0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          328192      reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 512)          2048        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          131328      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256)          1024        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 256)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256)          1024        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 256)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          32896       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128)          512         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 128)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          16512       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128)          512         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 8)            1032        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 8)            32          dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 8)            0           batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 585,064\n",
      "Trainable params: 582,488\n",
      "Non-trainable params: 2,576\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "input_layer = Input(name='input', shape=(input_dim, ))\n",
    "\n",
    "input_layer1 = Reshape((timesteps,64))(input_layer)#[10,64]\n",
    "att = TimeDistributed(Dense(64))(input_layer1)#[B,T,F]\n",
    "att = Activation('sigmoid')(att)\n",
    "\n",
    "att1 = Lambda(lambda x: K.sum(x, axis=-1))(att)\n",
    "att1 = Reshape([-1,1])(att1)\n",
    "att1 = Lambda(multA)(att1)\n",
    "att1 = Reshape([-1,64])(att1)\n",
    "\n",
    "att = Lambda(lambda x:x*64)(att)\n",
    "att1 = Lambda(lambda x:1/x)(att1)\n",
    "freq_fnorm = Multiply()([att,att1])\n",
    "\n",
    "h = Reshape((640,))(freq_fnorm)\n",
    "# h= Flatten()(cnn_input)\n",
    "# print(h.shape)\n",
    "h = Dense(512)(h)\n",
    "h = BatchNormalization()(h)\n",
    "h = Activation('relu')(h)\n",
    "\n",
    "h = Dense(256)(h)\n",
    "h = BatchNormalization()(h)\n",
    "h = Activation('relu')(h)\n",
    "\n",
    "h = Dense(256)(h)\n",
    "h = BatchNormalization()(h)\n",
    "h = Activation('relu')(h)\n",
    "\n",
    "h = Dense(128)(h)\n",
    "h = BatchNormalization()(h)\n",
    "h = Activation('relu')(h)\n",
    "\n",
    "h = Dense(128)(h)\n",
    "h = BatchNormalization()(h)\n",
    "h = Activation('relu')(h)\n",
    "\n",
    "h = Dense(8)(h)\n",
    "h = BatchNormalization()(h)\n",
    "h = Activation('relu')(h)\n",
    "\n",
    "  \n",
    "g_e = keras.models.Model(inputs=input_layer, outputs=h)\n",
    "\n",
    "\n",
    "g_e.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 8)                 585064    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 640)               328320    \n",
      "=================================================================\n",
      "Total params: 1,166,568\n",
      "Trainable params: 1,161,432\n",
      "Non-trainable params: 5,136\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(name='input', shape=(input_dim,))\n",
    "\n",
    "x = g_e(input_layer)\n",
    "\n",
    "\n",
    "y = Dense(128)(x)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "\n",
    "y = Dense(128)(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "\n",
    "y = Dense(256)(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "\n",
    "y = Dense(256)(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "\n",
    "y = Dense(512)(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "\n",
    "y = Dense(input_dim)(y)\n",
    "\n",
    "\n",
    "g = keras.models.Model(inputs=input_layer, outputs=y)\n",
    "\n",
    "g.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 640)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 10, 64)       0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 10, 64)       4160        reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 10, 64)       0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 10)           0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 10, 1)        0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 10, 64)       0           reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 10, 64)       0           lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 10, 64)       0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 10, 64)       0           reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 10, 64)       0           lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 10, 64)       0           reshape_5[0][0]                  \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 640)          0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 512)          328192      reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 512)          2048        dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 512)          0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 256)          131328      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 256)          1024        dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 256)          0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 256)          65792       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 256)          1024        dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 256)          0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 128)          32896       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 128)          512         dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128)          0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          16512       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128)          512         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 128)          0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 8)            1032        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8)            32          dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8)            0           batch_normalization_17[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 585,064\n",
      "Trainable params: 582,488\n",
      "Non-trainable params: 2,576\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(name='input', shape=(input_dim, ))\n",
    "\n",
    "input_layer1 = Reshape((timesteps,64))(input_layer)#[10,64]\n",
    "att = TimeDistributed(Dense(64))(input_layer1)#[B,T,F]\n",
    "att = Activation('sigmoid')(att)\n",
    "\n",
    "att1 = Lambda(lambda x: K.sum(x, axis=-1))(att)\n",
    "att1 = Reshape([-1,1])(att1)\n",
    "att1 = Lambda(multA)(att1)\n",
    "att1 = Reshape([-1,64])(att1)\n",
    "\n",
    "att = Lambda(lambda x:x*64)(att)\n",
    "att1 = Lambda(lambda x:1/x)(att1)\n",
    "freq_fnorm = Multiply()([att,att1])\n",
    "cnn_input = Multiply()([input_layer1,freq_fnorm])\n",
    "\n",
    "z = Reshape((640,))(cnn_input)\n",
    "\n",
    "z = Dense(512)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Activation('relu')(z)\n",
    "\n",
    "z = Dense(256)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Activation('relu')(z)\n",
    "\n",
    "z = Dense(256)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Activation('relu')(z)\n",
    "\n",
    "z = Dense(128)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Activation('relu')(z)\n",
    "\n",
    "z = Dense(128)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Activation('relu')(z)\n",
    "\n",
    "z = Dense(8)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Activation('relu')(z)\n",
    "\n",
    "encoder = keras.models.Model(input_layer, z)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               328192    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 513,024\n",
      "Trainable params: 510,976\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(name='input', shape=(input_dim,))\n",
    "\n",
    "f = Dense(512)(input_layer)\n",
    "f = BatchNormalization()(f)\n",
    "f = Activation('relu')(f)\n",
    "\n",
    "h = Dense(256)(h)\n",
    "h = BatchNormalization()(h)\n",
    "h = Activation('relu')(h)\n",
    "\n",
    "f = Dense(256)(f)\n",
    "f = BatchNormalization()(f)\n",
    "f = Activation('relu')(f)\n",
    "\n",
    "f = Dense(128)(f)\n",
    "f = BatchNormalization()(f)\n",
    "f = Activation('relu')(f)\n",
    "\n",
    "f = Dense(128)(f)\n",
    "f = BatchNormalization()(f)\n",
    "f = Activation('relu')(f)\n",
    "\n",
    "feature_extractor = keras.models.Model(input_layer, f)\n",
    "\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvLoss(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AdvLoss, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        ori_feature = feature_extractor(x[0])\n",
    "        gan_feature = feature_extractor(x[1])\n",
    "        return K.mean(K.square(ori_feature - K.mean(gan_feature, axis=0)))\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0][0], 1)\n",
    "    \n",
    "class CntLoss(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CntLoss, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        ori = x[0]\n",
    "        gan = x[1]\n",
    "        return K.mean(K.square(ori - gan))\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0][0], 1)\n",
    "    \n",
    "class EncLoss(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(EncLoss, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        ori = x[0]\n",
    "        gan = x[1]\n",
    "        return K.mean(K.square(g_e(ori) - encoder(gan)))\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0][0], 1)\n",
    "    \n",
    "# model for training\n",
    "input_layer = layers.Input(name='input', shape=(input_dim,))\n",
    "gan = g(input_layer) # g(x)\n",
    "\n",
    "adv_loss = AdvLoss(name='adv_loss')([input_layer, gan])\n",
    "cnt_loss = CntLoss(name='cnt_loss')([input_layer, gan])\n",
    "enc_loss = EncLoss(name='enc_loss')([input_layer, gan])\n",
    "\n",
    "gan_trainer = keras.models.Model(input_layer, [adv_loss, cnt_loss, enc_loss])\n",
    "\n",
    "# loss function\n",
    "def loss(yt, yp):\n",
    "    return yp\n",
    "\n",
    "losses = {\n",
    "    'adv_loss': loss,\n",
    "    'cnt_loss': loss,\n",
    "    'enc_loss': loss,\n",
    "}\n",
    "\n",
    "lossWeights = {'cnt_loss': 10.0, 'adv_loss': 1.0, 'enc_loss': 1.0}\n",
    "\n",
    "# compile\n",
    "gan_trainer.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=losses, loss_weights=lossWeights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 640)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 640)          1166568     input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "adv_loss (AdvLoss)              [(None, 640), (None, 0           input[0][0]                      \n",
      "                                                                 model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cnt_loss (CntLoss)              [(None, 640), (None, 0           input[0][0]                      \n",
      "                                                                 model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "enc_loss (EncLoss)              [(None, 640), (None, 0           input[0][0]                      \n",
      "                                                                 model_2[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,166,568\n",
      "Trainable params: 1,161,432\n",
      "Non-trainable params: 5,136\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan_trainer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "model_4 (Model)              (None, 128)               513024    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "d_out (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 517,313\n",
      "Trainable params: 515,201\n",
      "Non-trainable params: 2,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = layers.Input(name='input', shape=(input_dim,))\n",
    "\n",
    "f = feature_extractor(input_layer)\n",
    "\n",
    "d = Dense(32)(f)\n",
    "d = BatchNormalization()(d)\n",
    "d = Activation('elu')(d)\n",
    "\n",
    "d = layers.Dense(1, activation='sigmoid', name='d_out')(d)    \n",
    "\n",
    "d = keras.models.Model(input_layer, d)\n",
    "d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.compile(optimizer=keras.optimizers.Adam(lr=lr), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/3009 [00:00<?, ?it/s]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-27-26419edb6c49>\", line 47, in <module>\n",
      "    data = np.zeros((len(wav_list_c) * vectors.shape[0], dims), float)\n",
      "MemoryError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'MemoryError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Vibrastic\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Vibrastic\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Vibrastic\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Vibrastic\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\Vibrastic\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\Vibrastic\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(914736, 640)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ok.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vibrastic\\\\Documents'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mencari directory saat ini\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merubah directory\n",
    "os.chdir('D:\\Folder Pribadi Asisten\\Bagus\\Training')\n",
    "# Merubah directory (beneran)\n",
    "os.chdir('D:/Folder Pribadi Asisten/Bagus/Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3009/3009 [00:29<00:00, 102.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(914736, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 600/600 [00:05<00:00, 103.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.892547607421875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from keras.datasets import mnist\n",
    "# import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# additional\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import librosa.core\n",
    "import librosa.feature\n",
    "n_mels=64\n",
    "n_frames= 10\n",
    "n_hop_frames= 1\n",
    "n_fft= 1024\n",
    "hop_length= 512\n",
    "power= 2.0\n",
    "in_dir_c = 'D:/Folder Pribadi Asisten/Bagus/Training/pump/train/'\n",
    "wav_list_c = os.listdir(in_dir_c)\n",
    "idx = 0\n",
    "\n",
    "for  wav_file_c in tqdm(wav_list_c):     \n",
    "    if wav_file_c.endswith('.wav'):\n",
    "        indir_c1 = in_dir_c + wav_file_c\n",
    "        dims = n_mels * n_frames\n",
    "        y, sr = librosa.load(indir_c1, sr=None, mono=True)\n",
    "#         y -= np.mean(y)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=y,\n",
    "                                                     sr=sr,\n",
    "                                                     n_fft=n_fft,\n",
    "                                                     hop_length=hop_length,\n",
    "                                                     n_mels=n_mels,\n",
    "                                                     power=power)\n",
    "        log_mel_spectrogram = 20.0 / power * np.log10(np.maximum(mel_spectrogram, sys.float_info.epsilon))#(128, 313)\n",
    "#         log_mel_spectrogram = np.abs(log_mel_spectrogram)\n",
    "#         log_mel_spectrogram = np.mean(log_mel_spectrogram, axis=-1)\n",
    "        \n",
    "        n_vectors = len(log_mel_spectrogram[0, :]) - n_frames + 1\n",
    "#         print(n_vectors)\n",
    "        if n_vectors < 1:\n",
    "            vectors = np.empty((0, dims))\n",
    "        vectors = np.zeros((n_vectors, dims))#(250, 8192)\n",
    "        for t in range(n_frames):\n",
    "            vectors[:, n_mels * t : n_mels * (t + 1)] = log_mel_spectrogram[:, t : t + n_vectors].T\n",
    "        vectors = vectors[: : n_hop_frames, :]\n",
    "        if idx == 0:\n",
    "            data = np.zeros((len(wav_list_c) * vectors.shape[0], dims), float)\n",
    "        data[vectors.shape[0] * idx : vectors.shape[0] * (idx + 1), :] = vectors#(752250, 8192)\n",
    "        idx = 1+idx\n",
    "        \n",
    "        #print(data[0].shape)\n",
    "#x_ok = data.reshape(data.shape[0], 64, 128, 1)#[data.shape[0],64,128,1]\n",
    "x_ok = data\n",
    "print(x_ok.shape)\n",
    "\n",
    "i=0\n",
    "image = np.reshape(x_ok[i:i+1], (64, 10))\n",
    "# image = image * 127 + 127\n",
    "plt.imshow(image)  \n",
    "\n",
    "\n",
    "\n",
    "in_dir_c1 = 'D:/Folder Pribadi Asisten/Bagus/Training/valve/source_test/'\n",
    "wav_list_c1 = os.listdir(in_dir_c1)\n",
    "y_test = []\n",
    "idx = 0\n",
    "for wav_file_c in tqdm(wav_list_c1):    \n",
    "    if wav_file_c.endswith('.wav'):\n",
    "#         print(wav_file_c.split('_')[4])\n",
    "        if wav_file_c.split('_')[4] == 'anomaly':\n",
    "            for i in range(309):\n",
    "                y_test.append(0)\n",
    "        else :\n",
    "            for i in range(309):\n",
    "                y_test.append(1)\n",
    "        indir_c1 = in_dir_c1 + wav_file_c\n",
    "        dims = n_mels * n_frames\n",
    "        y, sr = librosa.load(indir_c1, sr=None, mono=True)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=y,\n",
    "                                                     sr=sr,\n",
    "                                                     n_fft=n_fft,\n",
    "                                                     hop_length=hop_length,\n",
    "                                                     n_mels=n_mels,\n",
    "                                                     power=power)\n",
    "        log_mel_spectrogram = 20.0 / power * np.log10(np.maximum(mel_spectrogram, sys.float_info.epsilon))#(128, 313)\n",
    "        n_vectors1 = len(log_mel_spectrogram[0, :]) - n_frames + 1\n",
    "        \n",
    "#         if n_vectors1 < 1:\n",
    "#             vectors1 = np.empty((0, dims))\n",
    "        vectors1 = np.zeros((n_vectors1, dims))#(250, 8192)\n",
    "        for t in range(n_frames):\n",
    "            vectors1[:, n_mels * t : n_mels * (t + 1)] = log_mel_spectrogram[:, t : t + n_vectors1].T\n",
    "        vectors1 = vectors1[: : n_hop_frames, :]\n",
    "        #print(vectors1.shape)\n",
    "        if idx == 0:\n",
    "            data1 = np.zeros((len(wav_list_c1) * vectors1.shape[0], dims), float)\n",
    "        \n",
    "        data1[vectors1.shape[0] * idx : vectors1.shape[0] * (idx + 1), :] = vectors1#(752250, 8192)\n",
    "        idx = 1+idx\n",
    "        \n",
    "x_test = data1.reshape(data1.shape[0], 64, 10, 1)#[data.shape[0],64,128,1]\n",
    "x_test = data1\n",
    "x_ok.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 20000\n",
    "bz = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_generator(data, batch_size=32):\n",
    "    datalen = len(data)\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        idxes = np.arange(datalen)\n",
    "        np.random.shuffle(idxes)\n",
    "        cnt += 1\n",
    "        for i in range(int(np.ceil(datalen/batch_size))):\n",
    "            train_x = np.take(data, idxes[i*batch_size: (i+1) * batch_size], axis=0)\n",
    "            y = np.ones(len(train_x))\n",
    "            yield train_x, [y, y, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = get_data_generator(x_ok, bz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'trainable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6b503cfb9040>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m33\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m### train disciminator ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mfake_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'trainable'"
     ]
    }
   ],
   "source": [
    "min = 10000\n",
    "for i in range(niter):\n",
    "    \n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        lr = 0.9\n",
    "        print(lr)\n",
    "    # get batch x, y ###\n",
    "    x, y = train_data_generator.__next__()\n",
    "    \n",
    "    d = 33\n",
    "    ### train disciminator ###\n",
    "    d.trainable = True\n",
    "        \n",
    "    fake_x = g.predict(x)\n",
    "#     print(lr)    \n",
    "    d_x = np.concatenate([x, fake_x], axis=0)\n",
    "    d_y = np.concatenate([np.zeros(len(x)), np.ones(len(fake_x))], axis=0)\n",
    "        \n",
    "    d_loss = d.train_on_batch(d_x, d_y)\n",
    "\n",
    "    ### train generator ###\n",
    "    \n",
    "    d.trainable = False        \n",
    "    g_loss = gan_trainer.train_on_batch(x, y)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        if(g_loss[2]<min):\n",
    "            min = g_loss[2]\n",
    "            #print(type(g_loss[2]))\n",
    "            print(g_loss[2],min)\n",
    "            g.save(\"model/model_pump.hdf5\")\n",
    "        print(f'niter: {i+1}, g_loss: {g_loss}, d_loss: {d_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
