{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8598ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, BatchNormalization, Activation\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, Concatenate\n",
    "from keras.layers import LeakyReLU, Dropout, Lambda, Activation, BatchNormalization\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels= 128\n",
    "n_frames= 64\n",
    "n_hop_frames= 8\n",
    "n_fft= 1024\n",
    "hop_length=512\n",
    "power= 2.0\n",
    "n_conditions = 3\n",
    "lr = 0.001\n",
    "\n",
    "def get_model(n_frames, n_mels, n_conditions, latent_dim=100):\n",
    "    in_image = Input(shape= (n_frames, n_mels, 3))\n",
    "    X = Conv2D(32, (3,3), strides=(2,2), padding='same')(in_image)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2D(64, (3,3), strides=(2,2), padding='same')(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2D(128, (3,3), strides=(2,2), padding='same')(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = Dropout(0.4)(X) #Consider adding more dropout layers to minimize overfitting - remember we work with limited labeled data. \n",
    "    X = Dense(latent_dim)(X)\n",
    "    \n",
    "    model = Model(inputs=in_image, outputs=X)\n",
    "    \n",
    "    return model\n",
    "\n",
    "enc_model= get_model(n_frames = n_frames, n_mels = n_mels, n_conditions = n_conditions)\n",
    "# print(enc_model.summary())\n",
    "\n",
    "def decoder(n_conditions, latent_dim = 100):\n",
    "\t\n",
    "\tin_lat = Input(shape=(latent_dim,))\n",
    "\t#Start with enough dense nodes to be reshaped and ConvTransposed to 28x28x1\n",
    "\tn_nodes = 256 * 16 * 32\n",
    "\tX = Dense(n_nodes)(in_lat)\n",
    "\tX = LeakyReLU(alpha=0.2)(X)\n",
    "\tX = Reshape((16, 32, 256))(X)\n",
    "\t\n",
    "\tX = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same')(X) #14x14x128\n",
    "\tX = LeakyReLU(alpha=0.2)(X)\n",
    "\t\n",
    "\tX = Conv2DTranspose(64, (3,3), strides=(1,1), padding='same')(X) #14x14x64\n",
    "\tX = LeakyReLU(alpha=0.2)(X)\n",
    "\t# output\n",
    "\tout_layer = Conv2DTranspose(n_conditions, (3,3), strides=(2,2), activation='tanh', \n",
    "                             padding='same')(X) #28x28x1\n",
    "\t# define model\n",
    "\tmodel = Model(in_lat, out_layer)\n",
    "\treturn model\n",
    "\n",
    "dec_model=decoder(n_conditions = n_conditions)\n",
    "# print(dec_model.summary())\n",
    "\n",
    "def define_generator(enc_model, dec_model):\n",
    "    # dec_model.trainable = False\n",
    "    gen_output = dec_model(enc_model.output) #Gen. output is the input to disc. \n",
    "    model = Model(enc_model.input, gen_output)\n",
    "    # model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "    return model\n",
    "\n",
    "gen_model = define_generator(enc_model, dec_model)\n",
    "# print(gen_model.summary())\n",
    "\n",
    "def define_discriminator(n_frames, n_mels, n_classes= n_conditions):\n",
    "    in_image = Input(shape=(n_frames, n_mels, 3))\n",
    "    X = Conv2D(32, (3,3), strides=(2,2), padding='same')(in_image)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2D(64, (3,3), strides=(2,2), padding='same')(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Conv2D(128, (3,3), strides=(2,2), padding='same')(X)\n",
    "    X = LeakyReLU(alpha=0.2)(X)\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = Dropout(0.4)(X) #Consider adding more dropout layers to minimize overfitting - remember we work with limited labeled data. \n",
    "    X = Dense(n_classes)(X)\n",
    "    \n",
    "    model = Model(inputs=in_image, outputs=X)\n",
    "    \n",
    "    return model\n",
    "\n",
    "disc_model=define_discriminator(n_frames = n_frames, n_mels = n_mels)\n",
    "# print(disc_model.summary())\n",
    "\n",
    "def define_gan(gen_model, disc_model):\n",
    "\t\n",
    "    disc_model.trainable = False # make unsup. discriminator not trainable\n",
    "    gan_output = disc_model(gen_model.output) #Gen. output is the input to disc. \n",
    "    model = Model(gen_model.input, gan_output)\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.00001, beta_1=0.5))\n",
    "    return model\n",
    "\n",
    "gan_model = define_gan(gen_model, disc_model)\n",
    "print(gan_model.summary())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856cfb65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
